// BufferContentInspector.m

#import "BufferContentInspector.h"
#import "logger.h"

@implementation BufferContentInspector {
    NSMutableArray *_capturedBufferInfo;
    NSUInteger _bufferCounter;
    dispatch_queue_t _processingQueue;
}

+ (instancetype)sharedInstance {
    static BufferContentInspector *sharedInstance = nil;
    static dispatch_once_t onceToken;
    dispatch_once(&onceToken, ^{
        sharedInstance = [[self alloc] init];
    });
    return sharedInstance;
}

- (instancetype)init {
    self = [super init];
    if (self) {
        _captureEnabled = YES;
        _analyzeContent = YES;
        _captureInterval = 100; // Capturar 1 a cada 100 frames
        _maxCapturedSamples = 50; // M√°ximo de amostras para n√£o ocupar muito espa√ßo
        _capturedBufferInfo = [NSMutableArray array];
        _bufferCounter = 0;
        _processingQueue = dispatch_queue_create("com.camera.buffer.inspection", DISPATCH_QUEUE_SERIAL);
        
        // Configurar diret√≥rio de sa√≠da
        NSString *documentsPath = NSSearchPathForDirectoriesInDomains(NSDocumentDirectory, NSUserDomainMask, YES).firstObject;
        _outputDirectory = [documentsPath stringByAppendingPathComponent:@"BufferSamples"];
        
        // Criar diret√≥rio se n√£o existir
        if (![[NSFileManager defaultManager] fileExistsAtPath:_outputDirectory]) {
            [[NSFileManager defaultManager] createDirectoryAtPath:_outputDirectory
                                      withIntermediateDirectories:YES
                                                       attributes:nil
                                                            error:nil];
        }
        
        LOG_INFO(@"BufferContentInspector inicializado. Diret√≥rio de amostras: %@", _outputDirectory);
    }
    return self;
}

#pragma mark - Captura de Amostras

- (void)captureSampleFromBuffer:(CMSampleBufferRef)sampleBuffer withContext:(NSString *)context {
    if (!_captureEnabled || !sampleBuffer) return;
    
    _bufferCounter++;
    
    // Verificar se deve capturar esta amostra
    if (_bufferCounter % _captureInterval != 0) return;
    
    // Verificar se j√° atingiu o limite de amostras
    if (_capturedBufferInfo.count >= _maxCapturedSamples) return;
    
    // Processar em fila ass√≠ncrona para n√£o bloquear o fluxo principal
    dispatch_async(_processingQueue, ^{
        // Capturar informa√ß√µes b√°sicas do buffer
        NSMutableDictionary *bufferInfo = [NSMutableDictionary dictionary];
        
        // ID e timestamp
        NSString *timestamp = [NSString stringWithFormat:@"%lld", (long long)([[NSDate date] timeIntervalSince1970] * 1000)];
        NSString *sampleID = [NSString stringWithFormat:@"sample_%llu_%@", (unsigned long long)_bufferCounter, timestamp];
        bufferInfo[@"sampleID"] = sampleID;
        bufferInfo[@"timestamp"] = timestamp;
        bufferInfo[@"context"] = context ?: @"unknown";
        
        // Informa√ß√µes do aplicativo
        bufferInfo[@"app"] = [[NSBundle mainBundle] bundleIdentifier];
        
        // Formato e dimens√µes
        CVImageBufferRef imageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer);
        if (imageBuffer) {
            size_t width = CVPixelBufferGetWidth(imageBuffer);
            size_t height = CVPixelBufferGetHeight(imageBuffer);
            OSType pixelFormat = CVPixelBufferGetPixelFormatType(imageBuffer);
            
            bufferInfo[@"width"] = @(width);
            bufferInfo[@"height"] = @(height);
            bufferInfo[@"pixelFormat"] = @(pixelFormat);
            
            char formatStr[5] = {0};
            formatStr[0] = (pixelFormat >> 24) & 0xFF;
            formatStr[1] = (pixelFormat >> 16) & 0xFF;
            formatStr[2] = (pixelFormat >> 8) & 0xFF;
            formatStr[3] = pixelFormat & 0xFF;
            bufferInfo[@"formatString"] = [NSString stringWithUTF8String:formatStr];
            
            // Extrair uma imagem do buffer
            NSString *imagePath = [self saveImageFromBuffer:imageBuffer withID:sampleID];
            if (imagePath) {
                bufferInfo[@"imagePath"] = imagePath;
            }
            
            // Analisar conte√∫do se habilitado
            if (_analyzeContent) {
                NSDictionary *contentAnalysis = [self analyzeBufferContent:sampleBuffer];
                if (contentAnalysis) {
                    bufferInfo[@"analysis"] = contentAnalysis;
                }
            }
        }
        
        // Salvar metadados em JSON
        NSString *metadataPath = [_outputDirectory stringByAppendingPathComponent:[NSString stringWithFormat:@"%@_metadata.json", sampleID]];
        NSData *jsonData = [NSJSONSerialization dataWithJSONObject:bufferInfo options:NSJSONWritingPretty error:nil];
        [jsonData writeToFile:metadataPath atomically:YES];
        
        // Adicionar ao array de informa√ß√µes capturadas
        [_capturedBufferInfo addObject:bufferInfo];
        _samplesSaved++;
        
        LOG_INFO(@"üì∏ Capturada amostra de buffer: %@", sampleID);
    });
}

- (NSString *)saveImageFromBuffer:(CVImageBufferRef)imageBuffer withID:(NSString *)sampleID {
    if (!imageBuffer) return nil;
    
    @try {
        // Verificar o formato e converter para UIImage
        OSType pixelFormat = CVPixelBufferGetPixelFormatType(imageBuffer);
        size_t width = CVPixelBufferGetWidth(imageBuffer);
        size_t height = CVPixelBufferGetHeight(imageBuffer);
        
        // Caminho do arquivo
        NSString *imagePath = [_outputDirectory stringByAppendingPathComponent:[NSString stringWithFormat:@"%@.png", sampleID]];
        
        // Lock do buffer para acesso
        CVPixelBufferLockBaseAddress(imageBuffer, kCVPixelBufferLock_ReadOnly);
        
        UIImage *image = nil;
        
        // Processar com base no formato
        if (pixelFormat == kCVPixelFormatType_32BGRA || 
            pixelFormat == kCVPixelFormatType_32RGBA) {
            
            CGColorSpaceRef colorSpace = CGColorSpaceCreateDeviceRGB();
            uint8_t *baseAddress = (uint8_t *)CVPixelBufferGetBaseAddress(imageBuffer);
            size_t bytesPerRow = CVPixelBufferGetBytesPerRow(imageBuffer);
            
            CGBitmapInfo bitmapInfo;
            if (pixelFormat == kCVPixelFormatType_32BGRA) {
                bitmapInfo = kCGBitmapByteOrder32Little | kCGImageAlphaPremultipliedFirst;
            } else { // 32RGBA
                bitmapInfo = kCGBitmapByteOrder32Big | kCGImageAlphaPremultipliedLast;
            }
            
            CGContextRef context = CGBitmapContextCreate(baseAddress, width, height, 8, bytesPerRow, colorSpace, bitmapInfo);
            
            if (context) {
                CGImageRef cgImage = CGBitmapContextCreateImage(context);
                if (cgImage) {
                    image = [UIImage imageWithCGImage:cgImage];
                    CGImageRelease(cgImage);
                }
                CGContextRelease(context);
            }
            CGColorSpaceRelease(colorSpace);
        }
        // Adicionar suporte para outros formatos comuns
        else if (pixelFormat == kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange ||
                 pixelFormat == kCVPixelFormatType_420YpCbCr8BiPlanarFullRange) {
            // Formatos YUV - precisa de convers√£o mais complexa
            // Esta √© uma abordagem simplificada que n√£o trata corretamente cores YUV
            // Em um ambiente de produ√ß√£o, use frameworks como GPUImage ou vImage
            
            // Criar um CIImage do buffer
            CIImage *ciImage = [CIImage imageWithCVPixelBuffer:imageBuffer];
            
            // Converter para UIImage via contexto
            CIContext *context = [CIContext contextWithOptions:nil];
            CGImageRef cgImage = [context createCGImage:ciImage fromRect:CGRectMake(0, 0, width, height)];
            
            if (cgImage) {
                image = [UIImage imageWithCGImage:cgImage];
                CGImageRelease(cgImage);
            }
        }
        
        CVPixelBufferUnlockBaseAddress(imageBuffer, kCVPixelBufferLock_ReadOnly);
        
        // Salvar a imagem como PNG
        if (image) {
            NSData *pngData = UIImagePNGRepresentation(image);
            [pngData writeToFile:imagePath atomically:YES];
            return imagePath;
        }
        
    } @catch (NSException *exception) {
        LOG_ERROR(@"Erro ao converter buffer para imagem: %@", exception);
    }
    
    return nil;
}

#pragma mark - An√°lise de Conte√∫do

- (NSDictionary *)analyzeBufferContent:(CMSampleBufferRef)sampleBuffer {
    if (!sampleBuffer) return nil;
    
    NSMutableDictionary *analysis = [NSMutableDictionary dictionary];
    _samplesAnalyzed++;
    
    @try {
        CVImageBufferRef imageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer);
        if (!imageBuffer) return nil;
        
        // Bloqueio para acesso
        CVPixelBufferLockBaseAddress(imageBuffer, kCVPixelBufferLock_ReadOnly);
        
        // 1. Calcular m√©dia e desvio padr√£o de luminosidade
        float avgBrightness = 0.0f;
        float stdDevBrightness = 0.0f;
        [self calculateBrightnessStats:imageBuffer average:&avgBrightness stdDev:&stdDevBrightness];
        
        analysis[@"avgBrightness"] = @(avgBrightness);
        analysis[@"stdDevBrightness"] = @(stdDevBrightness);
        
        // 2. Detec√ß√£o de caracter√≠sticas visuais b√°sicas
        NSDictionary *visualFeatures = [self extractVisualFeatures:sampleBuffer];
        if (visualFeatures) {
            [analysis addEntriesFromDictionary:visualFeatures];
        }
        
        // 3. Verificar se parece conte√∫do sint√©tico
        CGFloat confidence = 0.0f;
        BOOL isSynthetic = [self isLikelySyntheticContent:sampleBuffer confidence:&confidence];
        analysis[@"syntheticContentLikelihood"] = @(confidence);
        analysis[@"isSyntheticContent"] = @(isSynthetic);
        
        // Desbloquear buffer
        CVPixelBufferUnlockBaseAddress(imageBuffer, kCVPixelBufferLock_ReadOnly);
        
    } @catch (NSException *exception) {
        LOG_ERROR(@"Erro na an√°lise do buffer: %@", exception);
    }
    
    return analysis;
}

- (void)calculateBrightnessStats:(CVImageBufferRef)imageBuffer average:(float *)avgOut stdDev:(float *)stdDevOut {
    // Implementa√ß√£o b√°sica para BGRA
    OSType pixelFormat = CVPixelBufferGetPixelFormatType(imageBuffer);
    if (pixelFormat != kCVPixelFormatType_32BGRA) {
        // Simplificando - apenas um formato suportado neste exemplo
        *avgOut = 0.0f;
        *stdDevOut = 0.0f;
        return;
    }
    
    size_t width = CVPixelBufferGetWidth(imageBuffer);
    size_t height = CVPixelBufferGetHeight(imageBuffer);
    size_t bytesPerRow = CVPixelBufferGetBytesPerRow(imageBuffer);
    uint8_t *baseAddress = (uint8_t *)CVPixelBufferGetBaseAddress(imageBuffer);
    
    // Amostrar pixels em grade esparsa (evita processar todos os pixels)
    const size_t sampleStride = 16; // Analisa 1 a cada 16 pixels
    size_t sampleCount = 0;
    double sum = 0.0;
    double sumSq = 0.0;
    
    for (size_t y = 0; y < height; y += sampleStride) {
        for (size_t x = 0; x < width; x += sampleStride) {
            if (y >= height || x >= width) continue;
            
            size_t pixelOffset = y * bytesPerRow + x * 4;
            uint8_t b = baseAddress[pixelOffset];
            uint8_t g = baseAddress[pixelOffset + 1];
            uint8_t r = baseAddress[pixelOffset + 2];
            
            // F√≥rmula ponderada para luminosidade
            double luma = 0.299 * r + 0.587 * g + 0.114 * b;
            
            sum += luma;
            sumSq += luma * luma;
            sampleCount++;
        }
    }
    
    if (sampleCount > 0) {
        double mean = sum / sampleCount;
        double variance = (sumSq / sampleCount) - (mean * mean);
        variance = MAX(0.0, variance); // Evitar erros de arredondamento
        
        *avgOut = (float)mean / 255.0f; // Normaliza para [0, 1]
        *stdDevOut = (float)sqrt(variance) / 255.0f;
    } else {
        *avgOut = 0.0f;
        *stdDevOut = 0.0f;
    }
}

- (BOOL)isLikelySyntheticContent:(CMSampleBufferRef)sampleBuffer confidence:(CGFloat *)confidenceOut {
    // Esta √© uma implementa√ß√£o simplificada para detectar se um frame 
    // parece ser conte√∫do sint√©tico, como uma imagem est√°tica
    
    // Inicializar output
    if (confidenceOut) *confidenceOut = 0.0f;
    
    CVImageBufferRef imageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer);
    if (!imageBuffer) return NO;
    
    // Fatores que contribuem para a confian√ßa (totalizando 1.0)
    CGFloat stdDevWeight = 0.5f;  // Desvio padr√£o baixo = mais uniforme = mais sint√©tico 
    CGFloat edgeWeight = 0.3f;    // Poucas bordas = mais sint√©tico
    CGFloat noiseWeight = 0.2f;   // Pouco ru√≠do = mais sint√©tico
    
    // 1. Calcular uniformidade (baixo desvio padr√£o)
    float avgBrightness = 0.0f;
    float stdDevBrightness = 0.0f;
    [self calculateBrightnessStats:imageBuffer average:&avgBrightness stdDev:&stdDevBrightness];
    
    // Quanto menor o desvio padr√£o, mais uniforme (mais sint√©tico)
    CGFloat uniformityScore = 1.0f - MIN(1.0f, stdDevBrightness * 5.0f);
    
    // 2. Detec√ß√£o b√°sica de bordas
    NSDictionary *features = [self extractVisualFeatures:sampleBuffer];
    CGFloat edgeScore = 0.0f;
    CGFloat noiseScore = 0.0f;
    
    if (features) {
        NSNumber *edgeDensity = features[@"edgeDensity"];
        if (edgeDensity) {
            // Baixa densidade de bordas = mais sint√©tico
            edgeScore = 1.0f - MIN(1.0f, [edgeDensity floatValue] * 10.0f);
        }
        
        NSNumber *noiseLevel = features[@"noiseLevel"];
        if (noiseLevel) {
            // Baixo n√≠vel de ru√≠do = mais sint√©tico
            noiseScore = 1.0f - MIN(1.0f, [noiseLevel floatValue] * 5.0f);
        }
    }
    
    // Calcular confian√ßa combinada
    CGFloat confidence = (uniformityScore * stdDevWeight) + 
                         (edgeScore * edgeWeight) + 
                         (noiseScore * noiseWeight);
    
    if (confidenceOut) *confidenceOut = confidence;
    
    // Considerar sint√©tico se confian√ßa > 0.7 (ajust√°vel)
    return confidence > 0.7f;
}

- (NSDictionary *)extractVisualFeatures:(CMSampleBufferRef)sampleBuffer {
    // Extrair caracter√≠sticas visuais b√°sicas do buffer
    
    NSMutableDictionary *features = [NSMutableDictionary dictionary];
    CVImageBufferRef imageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer);
    if (!imageBuffer) return nil;
    
    OSType pixelFormat = CVPixelBufferGetPixelFormatType(imageBuffer);
    if (pixelFormat != kCVPixelFormatType_32BGRA) {
        // Simplificando - apenas um formato suportado
        return nil;
    }
    
    size_t width = CVPixelBufferGetWidth(imageBuffer);
    size_t height = CVPixelBufferGetHeight(imageBuffer);
    size_t bytesPerRow = CVPixelBufferGetBytesPerRow(imageBuffer);
    uint8_t *baseAddress = (uint8_t *)CVPixelBufferGetBaseAddress(imageBuffer);
    
    // Verifica√ß√µes b√°sicas
    if (width < 16 || height < 16) return nil;
    
    // An√°lise simplificada - estes s√£o algoritmos muito b√°sicos
    // Uma implementa√ß√£o real usaria bibliotecas como vImage ou OpenCV
    
    // 1. Calcular um histograma RGB simplificado (8 bins por canal)
    const int histogramBins = 8;
    int histogramR[histogramBins] = {0};
    int histogramG[histogramBins] = {0};
    int histogramB[histogramBins] = {0};
    
    // 2. Detec√ß√£o simplificada de bordas e ru√≠do
    int edgeCount = 0;
    float noiseSum = 0.0f;
    
    // Amostragem para histograma e caracter√≠sticas
    const size_t sampleStride = 8;
    
    for (size_t y = sampleStride; y < height - sampleStride; y += sampleStride) {
        for (size_t x = sampleStride; x < width - sampleStride; x += sampleStride) {
            size_t pixelOffset = y * bytesPerRow + x * 4;
            
            // Pixel atual
            uint8_t b = baseAddress[pixelOffset];
            uint8_t g = baseAddress[pixelOffset + 1];
            uint8_t r = baseAddress[pixelOffset + 2];
            
            // Histograma (agrupar em bins)
            histogramB[b * histogramBins / 256]++;
            histogramG[g * histogramBins / 256]++;
            histogramR[r * histogramBins / 256]++;
            
            // Detec√ß√£o simples de bordas (diferen√ßas com pixels vizinhos)
            size_t leftOffset = y * bytesPerRow + (x - sampleStride) * 4;
            size_t rightOffset = y * bytesPerRow + (x + sampleStride) * 4;
            size_t topOffset = (y - sampleStride) * bytesPerRow + x * 4;
            size_t bottomOffset = (y + sampleStride) * bytesPerRow + x * 4;
            
            // Calcular gradientes horizontais e verticais
            int gradientH = abs((int)baseAddress[leftOffset + 2] - (int)baseAddress[rightOffset + 2]) +
                           abs((int)baseAddress[leftOffset + 1] - (int)baseAddress[rightOffset + 1]) +
                           abs((int)baseAddress[leftOffset] - (int)baseAddress[rightOffset]);
            
            int gradientV = abs((int)baseAddress[topOffset + 2] - (int)baseAddress[bottomOffset + 2]) +
                           abs((int)baseAddress[topOffset + 1] - (int)baseAddress[bottomOffset + 1]) +
                           abs((int)baseAddress[topOffset] - (int)baseAddress[bottomOffset]);
            
            // Limiar para considerar uma borda
            int gradient = gradientH + gradientV;
            if (gradient > 100) { // Valor arbitr√°rio para o limiar
                edgeCount++;
            }
            
            // Estimativa de ru√≠do (varia√ß√£o local)
            noiseSum += gradient / 6.0f; // M√©dia das diferen√ßas absolutas
        }
    }
    
    // Normalizar contagem de bordas pelo tamanho da imagem
    float edgeDensity = (float)edgeCount / ((width * height) / (sampleStride * sampleStride));
    features[@"edgeDensity"] = @(edgeDensity);
    
    // Normalizar estimativa de ru√≠do
    float noiseLevel = noiseSum / ((width * height) / (sampleStride * sampleStride)) / 255.0f;
    features[@"noiseLevel"] = @(noiseLevel);
    
    // Histograma normalizado
    NSMutableArray *histogramData = [NSMutableArray array];
    for (int i = 0; i < histogramBins; i++) {
        [histogramData addObject:@{
            @"bin": @(i),
            @"r": @((float)histogramR[i] / ((width * height) / (sampleStride * sampleStride))),
            @"g": @((float)histogramG[i] / ((width * height) / (sampleStride * sampleStride))),
            @"b": @((float)histogramB[i] / ((width * height) / (sampleStride * sampleStride)))
        }];
    }
    
    features[@"histogram"] = histogramData;
    
    return features;
}

- (CGFloat)compareBuffer:(CMSampleBufferRef)buffer1 withBuffer:(CMSampleBufferRef)buffer2 {
    if (!buffer1 || !buffer2) return 1.0; // Diferentes se algum √© NULL
    
    CVImageBufferRef imageBuffer1 = CMSampleBufferGetImageBuffer(buffer1);
    CVImageBufferRef imageBuffer2 = CMSampleBufferGetImageBuffer(buffer2);
    
    if (!imageBuffer1 || !imageBuffer2) return 1.0;
    
    // Verificar dimens√µes
    size_t width1 = CVPixelBufferGetWidth(imageBuffer1);
    size_t height1 = CVPixelBufferGetHeight(imageBuffer1);
    size_t width2 = CVPixelBufferGetWidth(imageBuffer2);
    size_t height2 = CVPixelBufferGetHeight(imageBuffer2);
    
    if (width1 != width2 || height1 != height2) return 1.0; // Dimens√µes diferentes
    
    // Verificar formato
    OSType format1 = CVPixelBufferGetPixelFormatType(imageBuffer1);
    OSType format2 = CVPixelBufferGetPixelFormatType(imageBuffer2);
    
    if (format1 != format2) return 1.0; // Formatos diferentes
    
    // Simplifica√ß√£o: suportar apenas BGRA
    if (format1 != kCVPixelFormatType_32BGRA) return 1.0;
    
    // Bloquear buffers
    CVPixelBufferLockBaseAddress(imageBuffer1, kCVPixelBufferLock_ReadOnly);
    CVPixelBufferLockBaseAddress(imageBuffer2, kCVPixelBufferLock_ReadOnly);
    
    uint8_t *baseAddress1 = (uint8_t *)CVPixelBufferGetBaseAddress(imageBuffer1);
    uint8_t *baseAddress2 = (uint8_t *)CVPixelBufferGetBaseAddress(imageBuffer2);
    size_t bytesPerRow1 = CVPixelBufferGetBytesPerRow(imageBuffer1);
    size_t bytesPerRow2 = CVPixelBufferGetBytesPerRow(imageBuffer2);
    
    // Diferen√ßa m√©dia
    double totalDiff = 0.0;
    int sampleCount = 0;
    
    // Amostragem para compara√ß√£o (n√£o comparar todos os pixels)
    const size_t sampleStride = 8;
    
    for (size_t y = 0; y < height1; y += sampleStride) {
        for (size_t x = 0; x < width1; x += sampleStride) {
            size_t offset1 = y * bytesPerRow1 + x * 4;
            size_t offset2 = y * bytesPerRow2 + x * 4;
            
            // Diferen√ßa por canal
            int diffB = abs((int)baseAddress1[offset1] - (int)baseAddress2[offset2]);
            int diffG = abs((int)baseAddress1[offset1 + 1] - (int)baseAddress2[offset2 + 1]);
            int diffR = abs((int)baseAddress1[offset1 + 2] - (int)baseAddress2[offset2 + 2]);
            
            // Diferen√ßa m√©dia normalizada para este pixel
            double pixelDiff = (diffR + diffG + diffB) / (3.0 * 255.0);
            totalDiff += pixelDiff;
            sampleCount++;
        }
    }
    
    // Desbloquear buffers
    CVPixelBufferUnlockBaseAddress(imageBuffer1, kCVPixelBufferLock_ReadOnly);
    CVPixelBufferUnlockBaseAddress(imageBuffer2, kCVPixelBufferLock_ReadOnly);
    
    // Calcular diferen√ßa m√©dia normalizada
    double avgDiff = sampleCount > 0 ? totalDiff / sampleCount : 1.0;
    
    return (CGFloat)avgDiff;
}

- (NSDictionary *)detectContentPatterns:(CMSampleBufferRef)sampleBuffer {
    // Detectar padr√µes espec√≠ficos no conte√∫do (implementa√ß√£o b√°sica)
    NSMutableDictionary *patterns = [NSMutableDictionary dictionary];
    
    // Adicione aqui algoritmos para detectar padr√µes como:
    // - Teste de padr√µes (barras de cor, padr√£o de xadrez, etc.)
    // - Cenas est√°ticas vs. din√¢micas
    // - Presen√ßa de rosto
    // - Detec√ß√£o de texto
    
    // Simplifica√ß√£o: usar apenas informa√ß√µes j√° calculadas
    CGFloat confidence = 0.0f;
    BOOL isSynthetic = [self isLikelySyntheticContent:sampleBuffer confidence:&confidence];
    
    patterns[@"likelySynthetic"] = @(isSynthetic);
    patterns[@"syntheticConfidence"] = @(confidence);
    
    // Extrair caracter√≠sticas
    NSDictionary *features = [self extractVisualFeatures:sampleBuffer];
    if (features) {
        NSNumber *edgeDensity = features[@"edgeDensity"];
        if (edgeDensity && [edgeDensity floatValue] < 0.01) {
            patterns[@"likelyBlankScreen"] = @YES;
        } else if (edgeDensity && [edgeDensity floatValue] > 0.2) {
            patterns[@"likelyHighDetail"] = @YES;
        }
        
        // Outras detec√ß√µes com base no histograma
        NSArray *histogram = features[@"histogram"];
        if (histogram && [histogram count] > 0) {
            // Verificar se histograma mostra imagem muito brilhante
            float brightBins = 0;
            for (NSDictionary *bin in histogram) {
                NSInteger binIdx = [bin[@"bin"] integerValue];
                if (binIdx >= 6) { // 2 bins mais brilhantes (de 8)
                    brightBins += [bin[@"r"] floatValue] + [bin[@"g"] floatValue] + [bin[@"b"] floatValue];
                }
            }
            
            if (brightBins > 0.6) {
                patterns[@"likelyOverExposed"] = @YES;
            }
        }
    }
    
    return patterns;
}

- (NSString *)generateReport {
    NSMutableString *report = [NSMutableString string];
    
    [report appendString:@"===== RELAT√ìRIO DE INSPE√á√ÉO DE BUFFER =====\n\n"];
    [report appendFormat:@"Amostras analisadas: %lu\n", (unsigned long)_samplesAnalyzed];
    [report appendFormat:@"Amostras salvas: %lu\n", (unsigned long)_samplesSaved];
    [report appendFormat:@"Diret√≥rio de amostras: %@\n\n", _outputDirectory];
    
    // Estat√≠sticas por aplicativo
    NSMutableDictionary *statsByApp = [NSMutableDictionary dictionary];
    
    for (NSDictionary *sample in _capturedBufferInfo) {
        NSString *app = sample[@"app"];
        if (!app) continue;
        
        NSMutableDictionary *appStats = statsByApp[app];
        if (!appStats) {
            appStats = [NSMutableDictionary dictionary];
            appStats[@"count"] = @0;
            appStats[@"syntheticCount"] = @0;
            statsByApp[app] = appStats;
        }
        
        NSNumber *count = appStats[@"count"];
        appStats[@"count"] = @([count integerValue] + 1);
        
        // Verificar se sint√©tico
        NSDictionary *analysis = sample[@"analysis"];
        if (analysis && [analysis[@"isSyntheticContent"] boolValue]) {
            NSNumber *syntheticCount = appStats[@"syntheticCount"];
            appStats[@"syntheticCount"] = @([syntheticCount integerValue] + 1);
        }
    }
    
    [report appendString:@"Estat√≠sticas por aplicativo:\n"];
    for (NSString *app in statsByApp) {
        NSDictionary *appStats = statsByApp[app];
        NSInteger count = [appStats[@"count"] integerValue];
        NSInteger syntheticCount = [appStats[@"syntheticCount"] integerValue];
        
        [report appendFormat:@"- %@: %ld amostras, %.1f%% sint√©ticas\n", 
                 app, (long)count, (count > 0 ? (float)syntheticCount / count * 100 : 0)];
    }
    
    // An√°lise de formatos de pixel e dimens√µes
    [report appendString:@"\nAn√°lise de formatos:\n"];
    NSMutableDictionary *formatStats = [NSMutableDictionary dictionary];
    NSMutableDictionary *dimensionStats = [NSMutableDictionary dictionary];
    
    for (NSDictionary *sample in _capturedBufferInfo) {
        NSString *formatString = sample[@"formatString"];
        if (formatString) {
            NSNumber *count = formatStats[formatString];
            if (!count) count = @0;
            formatStats[formatString] = @([count integerValue] + 1);
        }
        
        NSNumber *width = sample[@"width"];
        NSNumber *height = sample[@"height"];
        if (width && height) {
            NSString *resolution = [NSString stringWithFormat:@"%@x%@", width, height];
            NSNumber *count = dimensionStats[resolution];
            if (!count) count = @0;
            dimensionStats[resolution] = @([count integerValue] + 1);
        }
    }
    
    // Mostrar formatos de pixel
    [report appendString:@"Formatos de pixel encontrados:\n"];
    for (NSString *format in formatStats) {
        NSNumber *count = formatStats[format];
        [report appendFormat:@"- '%@': %@ amostras (%.1f%%)\n", 
             format, count, (float)[count integerValue] / _samplesSaved * 100];
    }
    
    // Mostrar resolu√ß√µes
    [report appendString:@"\nResolu√ß√µes encontradas:\n"];
    for (NSString *resolution in dimensionStats) {
        NSNumber *count = dimensionStats[resolution];
        [report appendFormat:@"- %@: %@ amostras (%.1f%%)\n", 
             resolution, count, (float)[count integerValue] / _samplesSaved * 100];
    }
    
    // Resumo de caracter√≠sticas visuais
    float avgBrightness = 0.0f;
    float avgNoise = 0.0f;
    float avgEdges = 0.0f;
    NSInteger featuresCount = 0;
    
    for (NSDictionary *sample in _capturedBufferInfo) {
        NSDictionary *analysis = sample[@"analysis"];
        if (!analysis) continue;
        
        NSNumber *brightness = analysis[@"avgBrightness"];
        NSNumber *noiseLevel = analysis[@"noiseLevel"];
        NSNumber *edgeDensity = analysis[@"edgeDensity"];
        
        if (brightness && noiseLevel && edgeDensity) {
            avgBrightness += [brightness floatValue];
            avgNoise += [noiseLevel floatValue];
            avgEdges += [edgeDensity floatValue];
            featuresCount++;
        }
    }
    
    if (featuresCount > 0) {
        avgBrightness /= featuresCount;
        avgNoise /= featuresCount;
        avgEdges /= featuresCount;
        
        [report appendString:@"\nM√©dia das caracter√≠sticas visuais:\n"];
        [report appendFormat:@"- Brilho m√©dio: %.2f\n", avgBrightness];
        [report appendFormat:@"- N√≠vel de ru√≠do m√©dio: %.4f\n", avgNoise];
        [report appendFormat:@"- Densidade de bordas m√©dia: %.4f\n", avgEdges];
    }
    
    // Conclus√µes
    [report appendString:@"\nCONCLUS√ïES E RECOMENDA√á√ïES:\n"];
    
    // Verificar se h√° alta preval√™ncia de conte√∫do sint√©tico
    NSInteger totalSyntheticSamples = 0;
    for (NSString *app in statsByApp) {
        NSDictionary *appStats = statsByApp[app];
        totalSyntheticSamples += [appStats[@"syntheticCount"] integerValue];
    }
    
    float syntheticPercentage = _samplesSaved > 0 ? (float)totalSyntheticSamples / _samplesSaved * 100 : 0;
    
    if (syntheticPercentage > 50) {
        [report appendString:@"- ALERTA: Alta preval√™ncia de conte√∫do sint√©tico detectado (>50%).\n"];
        [report appendString:@"  Isso pode indicar que j√° h√° substitui√ß√£o de feed em andamento ou problemas no diagn√≥stico.\n"];
    }
    
    // Formato mais comum - poss√≠vel alvo para implementa√ß√£o
    NSString *mostCommonFormat = nil;
    NSInteger maxFormatCount = 0;
    for (NSString *format in formatStats) {
        NSInteger count = [formatStats[format] integerValue];
        if (count > maxFormatCount) {
            maxFormatCount = count;
            mostCommonFormat = format;
        }
    }
    
    if (mostCommonFormat) {
        [report appendFormat:@"- Formato mais comum para intercepta√ß√£o: '%@' (%.1f%% das amostras).\n", 
             mostCommonFormat, (float)maxFormatCount / _samplesSaved * 100];
    }
    
    // Resolu√ß√£o mais comum
    NSString *mostCommonResolution = nil;
    NSInteger maxResCount = 0;
    for (NSString *resolution in dimensionStats) {
        NSInteger count = [dimensionStats[resolution] integerValue];
        if (count > maxResCount) {
            maxResCount = count;
            mostCommonResolution = resolution;
        }
    }
    
    if (mostCommonResolution) {
        [report appendFormat:@"- Resolu√ß√£o mais comum para substitui√ß√£o: %@ (%.1f%% das amostras).\n", 
             mostCommonResolution, (float)maxResCount / _samplesSaved * 100];
    }
    
    [report appendString:@"\n==== FIM DO RELAT√ìRIO ====\n"];
    
    return report;
}

@end